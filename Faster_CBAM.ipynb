{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+u9MJh5NoLcyO494Onmbt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoSimon22/vision_artificial/blob/main/Faster_CBAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“˜ Faster R-CNN + CBAM con 1 clase (\"def\")\n",
        "Entrenamiento completo con dataset descargado desde Roboflow."
      ],
      "metadata": {
        "id": "N44FR2QBOueM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jebh_1AuNRld",
        "outputId": "9d59b8ca-5568-418c-dec7-7956be06f5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m143.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision pycocotools opencv-python matplotlib tqdm roboflow -q\n",
        "\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision import transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "!pip install pycocotools\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ğŸ”§ CBAM MÃ³dulo\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.fc(self.avg_pool(x)) + self.fc(self.max_pool(x))) * x\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        return self.sigmoid(self.conv(x_cat)) * x\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(in_planes, ratio)\n",
        "        self.sa = SpatialAttention(kernel_size)\n",
        "    def forward(self, x):\n",
        "        return self.sa(self.ca(x))\n",
        "\n",
        "# âš™ï¸ Cargar backbone con CBAM insertado correctamente\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "import types\n",
        "\n",
        "def insert_cbam_in_layer2(backbone, device):\n",
        "    original_layer2 = backbone.body.layer2\n",
        "    cbam = CBAM(512).to(device)\n",
        "\n",
        "    def new_forward(self, x):\n",
        "        x = original_layer2(x)\n",
        "        x = cbam(x)\n",
        "        return x\n",
        "\n",
        "    backbone.body.layer2.forward = types.MethodType(new_forward, backbone.body.layer2)\n",
        "    return backbone\n",
        "\n",
        "# ğŸ“¦ Crea el modelo con CBAM en layer2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
        "backbone = insert_cbam_in_layer2(backbone, device)\n",
        "model = FasterRCNN(backbone, num_classes=2)\n",
        "model.to(device)\n",
        "\n",
        "print(\"âœ… Modelo Faster R-CNN con CBAM integrado en layer2\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd17ytFANemH",
        "outputId": "942c7435-61bd-4556-8603-5efefd630bf2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 233MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Modelo Faster R-CNN con CBAM integrado en layer2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "import torch.nn as nn\n",
        "\n",
        "backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
        "backbone.body.layer2 = nn.Sequential(\n",
        "    backbone.body.layer2,\n",
        "    CBAM(512)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9nxmBisNVsp",
        "outputId": "1583b36e-cfe3-455c-831f-c112cc1fdbbb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FasterRCNN(backbone, num_classes=2)\n",
        "print(\"âœ… Modelo con CBAM cargado\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_N6zJgoNtLP",
        "outputId": "487002ba-5b4e-4129-d6fa-4a5bfe3cad57"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Modelo con CBAM cargado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"lX9sE2RPQOCv2R7cn0io\")\n",
        "project = rf.workspace(\"jota22\").project(\"am_boundingbox\")\n",
        "version = project.version(14)\n",
        "dataset = version.download(\"coco\")\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2pLHUJNy_U",
        "outputId": "bf5502cb-cc4d-4ac5-cd62-4705a4ae9799"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.64)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Requirement already satisfied: pillow-heif>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.22.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.58.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    print(f\"ğŸ“˜ Epoch [{epoch+1}] | Loss: {epoch_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fBFx_qn6N2-p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDetectionFasterRCNN(CocoDetection):\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super().__getitem__(idx)\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for obj in target:\n",
        "            x, y, w, h = obj['bbox']\n",
        "            if w > 0 and h > 0:\n",
        "                boxes.append([x, y, x + w, y + h])\n",
        "                labels.append(obj['category_id'])\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.tensor([], dtype=torch.int64)\n",
        "        else:\n",
        "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": torch.tensor([idx]),\n",
        "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
        "            \"iscrowd\": torch.zeros((len(labels),), dtype=torch.int64)\n",
        "        }\n",
        "        return img, target\n",
        "\n",
        "class SimpleTransform:\n",
        "    def __call__(self, image, target=None):\n",
        "        return T.ToTensor()(image), target\n",
        "\n",
        "def get_coco_dataset(img_dir, ann_file):\n",
        "    return CocoDetectionFasterRCNN(\n",
        "        root=img_dir,\n",
        "        annFile=ann_file,\n",
        "        transforms=SimpleTransform()\n",
        "    )\n",
        "\n",
        "train_dataset = get_coco_dataset(\n",
        "    img_dir=\"/content/AM_BoundingBox-14/train\",\n",
        "    ann_file=\"/content/AM_BoundingBox-14/train/_annotations.coco.json\"\n",
        ")\n",
        "\n",
        "val_dataset = get_coco_dataset(\n",
        "    img_dir=\"/content/AM_BoundingBox-14/valid\",\n",
        "    ann_file=\"/content/AM_BoundingBox-14/valid/_annotations.coco.json\"\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvE8Rpt4OExV",
        "outputId": "043e3945-c81a-4671-c00b-7002117485af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.20s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Sequential(\n",
              "          (0): Bottleneck(\n",
              "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "            )\n",
              "          )\n",
              "          (1): Bottleneck(\n",
              "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Bottleneck(\n",
              "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "          (3): Bottleneck(\n",
              "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (1): CBAM(\n",
              "          (ca): ChannelAttention(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
              "            (fc): Sequential(\n",
              "              (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): ReLU()\n",
              "              (2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            )\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "          (sa): SpatialAttention(\n",
              "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 150\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "    lr_scheduler.step()\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "        model_path = f\"fasterrcnn_resnet50_epoch_{epoch + 1}.pth\"\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"âœ… Model saved: {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2JhaiSkOT2U",
        "outputId": "f6fae502-522f-4ac3-8954-f17b0fce658e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“˜ Epoch [1] | Loss: 35.6812\n",
            "ğŸ“˜ Epoch [2] | Loss: 35.6715\n",
            "ğŸ“˜ Epoch [3] | Loss: 35.6869\n",
            "ğŸ“˜ Epoch [4] | Loss: 35.7342\n",
            "ğŸ“˜ Epoch [5] | Loss: 35.6432\n",
            "ğŸ“˜ Epoch [6] | Loss: 35.6836\n",
            "ğŸ“˜ Epoch [7] | Loss: 35.7144\n",
            "ğŸ“˜ Epoch [8] | Loss: 35.7020\n",
            "ğŸ“˜ Epoch [9] | Loss: 35.6646\n",
            "ğŸ“˜ Epoch [10] | Loss: 35.7124\n",
            "ğŸ“˜ Epoch [11] | Loss: 35.6926\n",
            "ğŸ“˜ Epoch [12] | Loss: 35.6798\n",
            "ğŸ“˜ Epoch [13] | Loss: 35.6358\n",
            "ğŸ“˜ Epoch [14] | Loss: 35.7107\n",
            "ğŸ“˜ Epoch [15] | Loss: 35.6733\n",
            "âœ… Model saved: fasterrcnn_resnet50_epoch_15.pth\n",
            "ğŸ“˜ Epoch [16] | Loss: 35.6559\n",
            "ğŸ“˜ Epoch [17] | Loss: 35.6570\n",
            "ğŸ“˜ Epoch [18] | Loss: 35.6936\n",
            "ğŸ“˜ Epoch [19] | Loss: 35.6683\n",
            "ğŸ“˜ Epoch [20] | Loss: 35.6808\n",
            "ğŸ“˜ Epoch [21] | Loss: 35.6924\n",
            "ğŸ“˜ Epoch [22] | Loss: 35.6406\n",
            "ğŸ“˜ Epoch [23] | Loss: 35.6729\n",
            "ğŸ“˜ Epoch [24] | Loss: 35.6740\n",
            "ğŸ“˜ Epoch [25] | Loss: 35.6699\n",
            "ğŸ“˜ Epoch [26] | Loss: 35.6384\n",
            "ğŸ“˜ Epoch [27] | Loss: 35.6607\n",
            "ğŸ“˜ Epoch [28] | Loss: 35.6895\n",
            "ğŸ“˜ Epoch [29] | Loss: 35.6609\n",
            "ğŸ“˜ Epoch [30] | Loss: 35.7007\n",
            "âœ… Model saved: fasterrcnn_resnet50_epoch_30.pth\n",
            "ğŸ“˜ Epoch [31] | Loss: 35.7333\n",
            "ğŸ“˜ Epoch [32] | Loss: 35.7293\n",
            "ğŸ“˜ Epoch [33] | Loss: 35.6686\n",
            "ğŸ“˜ Epoch [34] | Loss: 35.7159\n",
            "ğŸ“˜ Epoch [35] | Loss: 35.7071\n",
            "ğŸ“˜ Epoch [36] | Loss: 35.6877\n",
            "ğŸ“˜ Epoch [37] | Loss: 35.6736\n",
            "ğŸ“˜ Epoch [38] | Loss: 35.6788\n",
            "ğŸ“˜ Epoch [39] | Loss: 35.7104\n",
            "ğŸ“˜ Epoch [40] | Loss: 35.6777\n",
            "ğŸ“˜ Epoch [41] | Loss: 35.6793\n",
            "ğŸ“˜ Epoch [42] | Loss: 35.6664\n",
            "ğŸ“˜ Epoch [43] | Loss: 35.6852\n",
            "ğŸ“˜ Epoch [44] | Loss: 35.6580\n",
            "ğŸ“˜ Epoch [45] | Loss: 35.6917\n",
            "âœ… Model saved: fasterrcnn_resnet50_epoch_45.pth\n",
            "ğŸ“˜ Epoch [46] | Loss: 35.6484\n",
            "ğŸ“˜ Epoch [47] | Loss: 35.6608\n",
            "ğŸ“˜ Epoch [48] | Loss: 35.5608\n",
            "ğŸ“˜ Epoch [49] | Loss: 35.6266\n",
            "ğŸ“˜ Epoch [50] | Loss: 35.7321\n",
            "ğŸ“˜ Epoch [51] | Loss: 35.6936\n",
            "ğŸ“˜ Epoch [52] | Loss: 35.6864\n",
            "ğŸ“˜ Epoch [53] | Loss: 35.6445\n",
            "ğŸ“˜ Epoch [54] | Loss: 35.6494\n",
            "ğŸ“˜ Epoch [55] | Loss: 35.6846\n",
            "ğŸ“˜ Epoch [56] | Loss: 35.6607\n",
            "ğŸ“˜ Epoch [57] | Loss: 35.6670\n",
            "ğŸ“˜ Epoch [58] | Loss: 35.6647\n",
            "ğŸ“˜ Epoch [59] | Loss: 35.6834\n",
            "ğŸ“˜ Epoch [60] | Loss: 35.7147\n",
            "âœ… Model saved: fasterrcnn_resnet50_epoch_60.pth\n",
            "ğŸ“˜ Epoch [61] | Loss: 35.7503\n",
            "ğŸ“˜ Epoch [62] | Loss: 35.7005\n",
            "ğŸ“˜ Epoch [63] | Loss: 35.6757\n",
            "ğŸ“˜ Epoch [64] | Loss: 35.6993\n",
            "ğŸ“˜ Epoch [65] | Loss: 35.6386\n",
            "ğŸ“˜ Epoch [66] | Loss: 35.6557\n",
            "ğŸ“˜ Epoch [67] | Loss: 35.7015\n",
            "ğŸ“˜ Epoch [68] | Loss: 35.6942\n",
            "ğŸ“˜ Epoch [69] | Loss: 35.6983\n",
            "ğŸ“˜ Epoch [70] | Loss: 35.6826\n",
            "ğŸ“˜ Epoch [71] | Loss: 35.6990\n",
            "ğŸ“˜ Epoch [72] | Loss: 35.6576\n",
            "ğŸ“˜ Epoch [73] | Loss: 35.6896\n",
            "ğŸ“˜ Epoch [74] | Loss: 35.6519\n",
            "ğŸ“˜ Epoch [75] | Loss: 35.6924\n",
            "âœ… Model saved: fasterrcnn_resnet50_epoch_75.pth\n",
            "ğŸ“˜ Epoch [76] | Loss: 35.7036\n",
            "ğŸ“˜ Epoch [77] | Loss: 35.7148\n",
            "ğŸ“˜ Epoch [78] | Loss: 35.6128\n",
            "ğŸ“˜ Epoch [79] | Loss: 35.6603\n",
            "ğŸ“˜ Epoch [80] | Loss: 35.6987\n",
            "ğŸ“˜ Epoch [81] | Loss: 35.6923\n",
            "ğŸ“˜ Epoch [82] | Loss: 35.6556\n",
            "ğŸ“˜ Epoch [83] | Loss: 35.6714\n",
            "ğŸ“˜ Epoch [84] | Loss: 35.6669\n",
            "ğŸ“˜ Epoch [85] | Loss: 35.6440\n",
            "ğŸ“˜ Epoch [86] | Loss: 35.6756\n",
            "ğŸ“˜ Epoch [87] | Loss: 35.6572\n",
            "ğŸ“˜ Epoch [88] | Loss: 35.6659\n",
            "ğŸ“˜ Epoch [89] | Loss: 35.6448\n",
            "ğŸ“˜ Epoch [90] | Loss: 35.6957\n",
            "âœ… Model saved: fasterrcnn_resnet50_epoch_90.pth\n",
            "ğŸ“˜ Epoch [91] | Loss: 35.7225\n",
            "ğŸ“˜ Epoch [92] | Loss: 35.6930\n",
            "ğŸ“˜ Epoch [93] | Loss: 35.6267\n",
            "ğŸ“˜ Epoch [94] | Loss: 35.6842\n",
            "ğŸ“˜ Epoch [95] | Loss: 35.7324\n",
            "ğŸ“˜ Epoch [96] | Loss: 35.6110\n",
            "ğŸ“˜ Epoch [97] | Loss: 35.6232\n",
            "ğŸ“˜ Epoch [98] | Loss: 35.6792\n",
            "ğŸ“˜ Epoch [99] | Loss: 35.6576\n",
            "ğŸ“˜ Epoch [100] | Loss: 35.5852\n",
            "ğŸ“˜ Epoch [101] | Loss: 35.6813\n",
            "ğŸ“˜ Epoch [102] | Loss: 35.6993\n",
            "ğŸ“˜ Epoch [103] | Loss: 35.6741\n",
            "ğŸ“˜ Epoch [104] | Loss: 35.7190\n",
            "ğŸ“˜ Epoch [105] | Loss: 35.6948\n",
            "âœ… Model saved: fasterrcnn_resnet50_epoch_105.pth\n",
            "ğŸ“˜ Epoch [106] | Loss: 35.6558\n",
            "ğŸ“˜ Epoch [107] | Loss: 35.7039\n",
            "ğŸ“˜ Epoch [108] | Loss: 35.7270\n",
            "ğŸ“˜ Epoch [109] | Loss: 35.7014\n",
            "ğŸ“˜ Epoch [110] | Loss: 35.6789\n",
            "ğŸ“˜ Epoch [111] | Loss: 35.6481\n",
            "ğŸ“˜ Epoch [112] | Loss: 35.6599\n",
            "ğŸ“˜ Epoch [113] | Loss: 35.7010\n",
            "ğŸ“˜ Epoch [114] | Loss: 35.6295\n",
            "ğŸ“˜ Epoch [115] | Loss: 35.6781\n",
            "ğŸ“˜ Epoch [116] | Loss: 35.6523\n",
            "ğŸ“˜ Epoch [117] | Loss: 35.6697\n",
            "ğŸ“˜ Epoch [118] | Loss: 35.6837\n",
            "ğŸ“˜ Epoch [119] | Loss: 35.6350\n",
            "ğŸ“˜ Epoch [120] | Loss: 35.6601\n",
            "âœ… Model saved: fasterrcnn_resnet50_epoch_120.pth\n",
            "ğŸ“˜ Epoch [121] | Loss: 35.7197\n",
            "ğŸ“˜ Epoch [122] | Loss: 35.6931\n",
            "ğŸ“˜ Epoch [123] | Loss: 35.6640\n",
            "ğŸ“˜ Epoch [124] | Loss: 35.6432\n",
            "ğŸ“˜ Epoch [125] | Loss: 35.6935\n",
            "ğŸ“˜ Epoch [126] | Loss: 35.6660\n",
            "ğŸ“˜ Epoch [127] | Loss: 35.7073\n",
            "ğŸ“˜ Epoch [128] | Loss: 35.6405\n",
            "ğŸ“˜ Epoch [129] | Loss: 35.6893\n",
            "ğŸ“˜ Epoch [130] | Loss: 35.6936\n",
            "ğŸ“˜ Epoch [131] | Loss: 35.6743\n",
            "ğŸ“˜ Epoch [132] | Loss: 35.6456\n",
            "ğŸ“˜ Epoch [133] | Loss: 35.6470\n",
            "ğŸ“˜ Epoch [134] | Loss: 35.6638\n",
            "ğŸ“˜ Epoch [135] | Loss: 35.6747\n",
            "âœ… Model saved: fasterrcnn_resnet50_epoch_135.pth\n",
            "ğŸ“˜ Epoch [136] | Loss: 35.7403\n",
            "ğŸ“˜ Epoch [137] | Loss: 35.7038\n",
            "ğŸ“˜ Epoch [138] | Loss: 35.6901\n",
            "ğŸ“˜ Epoch [139] | Loss: 35.6930\n",
            "ğŸ“˜ Epoch [140] | Loss: 35.6900\n",
            "ğŸ“˜ Epoch [141] | Loss: 35.6961\n",
            "ğŸ“˜ Epoch [142] | Loss: 35.7259\n",
            "ğŸ“˜ Epoch [143] | Loss: 35.6731\n",
            "ğŸ“˜ Epoch [144] | Loss: 35.6849\n",
            "ğŸ“˜ Epoch [145] | Loss: 35.6823\n",
            "ğŸ“˜ Epoch [146] | Loss: 35.6762\n",
            "ğŸ“˜ Epoch [147] | Loss: 35.7110\n",
            "ğŸ“˜ Epoch [148] | Loss: 35.6729\n",
            "ğŸ“˜ Epoch [149] | Loss: 35.7023\n",
            "ğŸ“˜ Epoch [150] | Loss: 35.6501\n",
            "âœ… Model saved: fasterrcnn_resnet50_epoch_150.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ” EvaluaciÃ³n con pycocotools sobre el conjunto de test\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Cargar dataset de test\n",
        "from torchvision.datasets import CocoDetection\n",
        "\n",
        "test_img_dir = \"/content/AM_BoundingBox-14/test\"\n",
        "test_ann_file = \"/content/AM_BoundingBox-14/test/_annotations.coco.json\"\n",
        "\n",
        "class CocoDetectionTest(CocoDetection):\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super().__getitem__(idx)\n",
        "        image_id = target[0]['image_id'] if target else idx\n",
        "        return img, image_id\n",
        "\n",
        "test_dataset = CocoDetectionTest(test_img_dir, test_ann_file, transform=T.ToTensor())\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, image_ids in test_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        outputs = model(images)\n",
        "\n",
        "        for output, image_id in zip(outputs, image_ids):\n",
        "            boxes = output['boxes'].cpu().numpy()\n",
        "            scores = output['scores'].cpu().numpy()\n",
        "            labels = output['labels'].cpu().numpy()\n",
        "\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                x_min, y_min, x_max, y_max = box\n",
        "                results.append({\n",
        "                    \"image_id\": int(image_id),\n",
        "                    \"category_id\": int(label),\n",
        "                    \"bbox\": [float(x_min), float(y_min), float(x_max - x_min), float(y_max - y_min)],\n",
        "                    \"score\": float(score)\n",
        "                })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssGxB5eNOU2g",
        "outputId": "9a7f6999-b30c-4c9b-adb0-6de54a52db5d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar resultados en JSON para COCOeval\n",
        "with open(\"results_coco.json\", \"w\") as f:\n",
        "    json.dump(results, f)\n",
        "\n",
        "coco_gt = COCO(test_ann_file)\n",
        "coco_dt = coco_gt.loadRes(\"results_coco.json\")\n",
        "\n",
        "coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
        "coco_eval.evaluate()\n",
        "coco_eval.accumulate()\n",
        "coco_eval.summarize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8abQSxtUOjhR",
        "outputId": "d86ebcdd-2623-4223-a402-74bfd9247066"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.74s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.017\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
          ]
        }
      ]
    }
  ]
}