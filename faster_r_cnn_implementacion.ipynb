{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNScRQp3Z2KAmuuOEiAtVU4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoSimon22/vision_artificial/blob/main/faster_r_cnn_implementacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGaeq4ADgft2",
        "outputId": "a51e47ea-31d7-4455-869f-57fce197ff6f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.58-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.56.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Downloading roboflow-1.1.58-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.58\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in AM_BoundingBox-14 to coco:: 100%|██████████| 6817/6817 [00:00<00:00, 7276.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to AM_BoundingBox-14 in coco:: 100%|██████████| 198/198 [00:00<00:00, 5691.95it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.transforms import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"lX9sE2RPQOCv2R7cn0io\")\n",
        "project = rf.workspace(\"jota22\").project(\"am_boundingbox\")\n",
        "version = project.version(14)\n",
        "dataset = version.download(\"coco\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "class CocoTransform:\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)  # Convert PIL image to tensor\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "870v0WUDg-x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "def get_coco_dataset(img_dir, ann_file):\n",
        "    return CocoDetection(\n",
        "        root=img_dir,\n",
        "        annFile=ann_file,\n",
        "        transforms=CocoTransform()\n",
        "    )\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = get_coco_dataset(\n",
        "    img_dir=\"/content/AM_BoundingBox-14/train\",\n",
        "    ann_file=\"/content/AM_BoundingBox-14/train/_annotations.coco.json\"\n",
        ")\n",
        "\n",
        "\n",
        "val_dataset = get_coco_dataset(\n",
        "    img_dir=\"/content/AM_BoundingBox-14/valid\",\n",
        "    ann_file=\"/content/AM_BoundingBox-14/valid/_annotations.coco.json\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SUiJysJhB2D",
        "outputId": "c0ed6e5d-af18-4adc-f391-046450a13a7e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.09s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Faster R-CNN with ResNet-50 backbone\n",
        "def get_model(num_classes):\n",
        "    # Load pre-trained Faster R-CNN\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # Get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # Replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "W6HeavRto22C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "num_classes = 2 # background + def\n",
        "model = get_model(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8kVDOHko-mV",
        "outputId": "ad785f23-27ae-4d1e-d9c9-b8616a1835c0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:00<00:00, 187MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
      ],
      "metadata": {
        "id": "8aiBeYNPpF_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    for images, targets in data_loader:\n",
        "        # Move images to the device\n",
        "        images = [img.to(device) for img in images]\n",
        "\n",
        "        # Validate and process targets\n",
        "        processed_targets = []\n",
        "        valid_images = []\n",
        "        for i, target in enumerate(targets):\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            for obj in target:\n",
        "                # Extract bbox\n",
        "                bbox = obj[\"bbox\"]  # Format: [x, y, width, height]\n",
        "                x, y, w, h = bbox\n",
        "\n",
        "                # Ensure the width and height are positive\n",
        "                if w > 0 and h > 0:\n",
        "                    boxes.append([x, y, x + w, y + h])  # Convert to [x_min, y_min, x_max, y_max]\n",
        "                    labels.append(obj[\"category_id\"])\n",
        "\n",
        "            # Only process if there are valid boxes\n",
        "            if boxes:\n",
        "                processed_target = {\n",
        "                    \"boxes\": torch.tensor(boxes, dtype=torch.float32).to(device),\n",
        "                    \"labels\": torch.tensor(labels, dtype=torch.int64).to(device),\n",
        "                }\n",
        "                processed_targets.append(processed_target)\n",
        "                valid_images.append(images[i])  # Add only valid images\n",
        "\n",
        "        # Skip iteration if no valid targets\n",
        "        if not processed_targets:\n",
        "            continue\n",
        "\n",
        "        # Ensure images and targets are aligned\n",
        "        images = valid_images\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, processed_targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch}] Loss: {losses.item():.4f}\")"
      ],
      "metadata": {
        "id": "YThPDcQUpQMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 150\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Save the model's state dictionary every 10 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "        model_path = f\"fasterrcnn_resnet50_epoch_{epoch + 1}.pth\"\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"✅ Model saved: {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGP1pCn5pbx6",
        "outputId": "64df3edb-05a1-4688-899e-438edcac3334",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0] Loss: 0.9778\n",
            "Epoch [1] Loss: 1.0183\n",
            "Epoch [2] Loss: 0.9870\n",
            "Epoch [3] Loss: 0.9042\n",
            "Epoch [4] Loss: 0.8954\n",
            "Epoch [5] Loss: 1.0818\n",
            "Epoch [6] Loss: 0.9317\n",
            "Epoch [7] Loss: 1.0317\n",
            "Epoch [8] Loss: 1.2131\n",
            "Epoch [9] Loss: 0.9586\n",
            "Epoch [10] Loss: 1.1286\n",
            "Epoch [11] Loss: 0.9955\n",
            "Epoch [12] Loss: 1.0205\n",
            "Epoch [13] Loss: 0.9911\n",
            "Epoch [14] Loss: 0.9473\n",
            "✅ Model saved: fasterrcnn_resnet50_epoch_15.pth\n",
            "Epoch [15] Loss: 0.9657\n",
            "Epoch [16] Loss: 1.0088\n",
            "Epoch [17] Loss: 0.9860\n",
            "Epoch [18] Loss: 0.9864\n",
            "Epoch [19] Loss: 0.8537\n",
            "Epoch [20] Loss: 0.6659\n",
            "Epoch [21] Loss: 0.8754\n",
            "Epoch [22] Loss: 0.9667\n",
            "Epoch [23] Loss: 1.1067\n",
            "Epoch [24] Loss: 1.1261\n",
            "Epoch [25] Loss: 0.9425\n",
            "Epoch [26] Loss: 1.0165\n",
            "Epoch [27] Loss: 1.0123\n",
            "Epoch [28] Loss: 1.0575\n",
            "Epoch [29] Loss: 0.9797\n",
            "✅ Model saved: fasterrcnn_resnet50_epoch_30.pth\n",
            "Epoch [30] Loss: 0.9357\n",
            "Epoch [31] Loss: 1.0195\n",
            "Epoch [32] Loss: 1.0534\n",
            "Epoch [33] Loss: 1.2621\n",
            "Epoch [34] Loss: 0.9765\n",
            "Epoch [35] Loss: 1.0557\n",
            "Epoch [36] Loss: 1.0910\n",
            "Epoch [37] Loss: 1.0311\n",
            "Epoch [38] Loss: 0.8109\n",
            "Epoch [39] Loss: 0.9418\n",
            "Epoch [40] Loss: 0.9554\n",
            "Epoch [41] Loss: 1.0706\n",
            "Epoch [42] Loss: 0.9329\n",
            "Epoch [43] Loss: 1.0266\n",
            "Epoch [44] Loss: 1.0138\n",
            "✅ Model saved: fasterrcnn_resnet50_epoch_45.pth\n",
            "Epoch [45] Loss: 0.7766\n",
            "Epoch [46] Loss: 0.8671\n",
            "Epoch [47] Loss: 1.0277\n",
            "Epoch [48] Loss: 1.1127\n",
            "Epoch [49] Loss: 1.0318\n",
            "Epoch [50] Loss: 0.6885\n",
            "Epoch [51] Loss: 0.9705\n",
            "Epoch [52] Loss: 0.9914\n",
            "Epoch [53] Loss: 1.0456\n",
            "Epoch [54] Loss: 1.0302\n",
            "Epoch [55] Loss: 0.7111\n",
            "Epoch [56] Loss: 1.0596\n",
            "Epoch [57] Loss: 1.0164\n",
            "Epoch [58] Loss: 0.9951\n",
            "Epoch [59] Loss: 0.9315\n",
            "✅ Model saved: fasterrcnn_resnet50_epoch_60.pth\n",
            "Epoch [60] Loss: 1.0903\n",
            "Epoch [61] Loss: 0.9495\n",
            "Epoch [62] Loss: 1.1187\n",
            "Epoch [63] Loss: 0.9859\n",
            "Epoch [64] Loss: 1.0064\n",
            "Epoch [65] Loss: 0.9786\n",
            "Epoch [66] Loss: 1.0167\n",
            "Epoch [67] Loss: 0.9579\n",
            "Epoch [68] Loss: 0.9627\n",
            "Epoch [69] Loss: 0.9833\n",
            "Epoch [70] Loss: 1.1193\n",
            "Epoch [71] Loss: 0.9706\n",
            "Epoch [72] Loss: 1.0141\n",
            "Epoch [73] Loss: 1.1076\n",
            "Epoch [74] Loss: 1.0800\n",
            "✅ Model saved: fasterrcnn_resnet50_epoch_75.pth\n",
            "Epoch [75] Loss: 1.0124\n",
            "Epoch [76] Loss: 1.2053\n",
            "Epoch [77] Loss: 1.0417\n",
            "Epoch [78] Loss: 1.0507\n",
            "Epoch [79] Loss: 0.9696\n",
            "Epoch [80] Loss: 0.8795\n",
            "Epoch [81] Loss: 1.0372\n",
            "Epoch [82] Loss: 0.9634\n",
            "Epoch [83] Loss: 0.9899\n",
            "Epoch [84] Loss: 1.1049\n",
            "Epoch [85] Loss: 1.0398\n",
            "Epoch [86] Loss: 0.6578\n",
            "Epoch [87] Loss: 1.0120\n",
            "Epoch [88] Loss: 1.0893\n",
            "Epoch [89] Loss: 1.0086\n",
            "✅ Model saved: fasterrcnn_resnet50_epoch_90.pth\n",
            "Epoch [90] Loss: 0.9515\n",
            "Epoch [91] Loss: 1.0290\n",
            "Epoch [92] Loss: 0.9557\n",
            "Epoch [93] Loss: 1.0416\n",
            "Epoch [94] Loss: 1.0616\n",
            "Epoch [95] Loss: 1.1299\n",
            "Epoch [96] Loss: 0.9791\n",
            "Epoch [97] Loss: 1.0802\n",
            "Epoch [98] Loss: 0.9720\n",
            "Epoch [99] Loss: 1.0905\n",
            "Epoch [100] Loss: 0.9351\n",
            "Epoch [101] Loss: 1.0125\n",
            "Epoch [102] Loss: 1.0562\n",
            "Epoch [103] Loss: 0.9816\n",
            "Epoch [104] Loss: 0.9919\n",
            "✅ Model saved: fasterrcnn_resnet50_epoch_105.pth\n",
            "Epoch [105] Loss: 1.0088\n",
            "Epoch [106] Loss: 1.0022\n",
            "Epoch [107] Loss: 0.9721\n",
            "Epoch [108] Loss: 0.9196\n",
            "Epoch [109] Loss: 1.0562\n",
            "Epoch [110] Loss: 0.9642\n",
            "Epoch [111] Loss: 0.9130\n",
            "Epoch [112] Loss: 1.0642\n",
            "Epoch [113] Loss: 0.9803\n",
            "Epoch [114] Loss: 1.0831\n",
            "Epoch [115] Loss: 1.0995\n",
            "Epoch [116] Loss: 0.7187\n",
            "Epoch [117] Loss: 1.0156\n",
            "Epoch [118] Loss: 0.9916\n",
            "Epoch [119] Loss: 0.9740\n",
            "✅ Model saved: fasterrcnn_resnet50_epoch_120.pth\n",
            "Epoch [120] Loss: 1.0107\n",
            "Epoch [121] Loss: 1.0149\n",
            "Epoch [122] Loss: 1.0102\n",
            "Epoch [123] Loss: 1.0362\n",
            "Epoch [124] Loss: 0.9512\n",
            "Epoch [125] Loss: 0.9668\n",
            "Epoch [126] Loss: 0.7020\n",
            "Epoch [127] Loss: 1.0293\n",
            "Epoch [128] Loss: 1.1179\n",
            "Epoch [129] Loss: 1.0213\n",
            "Epoch [130] Loss: 0.9822\n",
            "Epoch [131] Loss: 0.9524\n",
            "Epoch [132] Loss: 1.0036\n",
            "Epoch [133] Loss: 0.9491\n",
            "Epoch [134] Loss: 1.0422\n",
            "✅ Model saved: fasterrcnn_resnet50_epoch_135.pth\n",
            "Epoch [135] Loss: 1.1017\n",
            "Epoch [136] Loss: 1.0465\n",
            "Epoch [137] Loss: 1.0585\n",
            "Epoch [138] Loss: 0.9660\n",
            "Epoch [139] Loss: 1.0212\n",
            "Epoch [140] Loss: 0.9494\n",
            "Epoch [141] Loss: 1.0930\n",
            "Epoch [142] Loss: 1.0076\n",
            "Epoch [143] Loss: 0.9404\n",
            "Epoch [144] Loss: 1.0472\n",
            "Epoch [145] Loss: 1.0108\n",
            "Epoch [146] Loss: 0.9219\n",
            "Epoch [147] Loss: 0.9400\n",
            "Epoch [148] Loss: 0.7116\n",
            "Epoch [149] Loss: 0.7183\n",
            "✅ Model saved: fasterrcnn_resnet50_epoch_150.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MEJORADO: Evaluar todas las imágenes del conjunto de validación y visualizar ---\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURACIÓN ---\n",
        "num_classes = 2  # 1 clase + fondo\n",
        "model_path = \"/content/fasterrcnn_resnet50_epoch_90.pth\"\n",
        "img_dir = \"/content/AM_BoundingBox-14/test\"  # <-- Cambia esto si es necesario\n",
        "ann_file = \"/content/AM_BoundingBox-14/test/_annotations.coco.json\"\n",
        "\n",
        "# --- CARGAR MODELO ---\n",
        "def get_model(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_model(num_classes)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# --- COCO DATASET & LOADER ---\n",
        "def get_coco_dataset(img_dir, ann_file):\n",
        "    return CocoDetection(root=img_dir, annFile=ann_file)\n",
        "\n",
        "dataset = get_coco_dataset(img_dir, ann_file)\n",
        "val_loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# --- FUNCIONES DE DIBUJO ---\n",
        "COCO_CLASSES = {0: \"background\", 1: \"def\"}\n",
        "\n",
        "def get_class_name(class_id):\n",
        "    return COCO_CLASSES.get(class_id, \"unknown\")\n",
        "\n",
        "def draw_boxes(image, prediction, threshold=0.5):\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    boxes = prediction[\"boxes\"].cpu().numpy()\n",
        "    labels = prediction[\"labels\"].cpu().numpy()\n",
        "    scores = prediction[\"scores\"].cpu().numpy()\n",
        "\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        if score >= threshold:\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            draw.rectangle([x_min, y_min, x_max, y_max], outline=\"red\", width=2)\n",
        "            draw.text((x_min, y_min), f\"{get_class_name(label)} ({score:.2f})\", fill=\"red\")\n",
        "    return image\n",
        "\n",
        "# --- EVALUAR Y GUARDAR RESULTADOS DE TODAS LAS IMÁGENES ---\n",
        "out_dir = \"detections\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (images, targets) in enumerate(tqdm(val_loader, desc=\"Evaluando\")):\n",
        "        image_tensor = images[0].to(device)\n",
        "        output = model([image_tensor])[0]\n",
        "\n",
        "        # Cargar imagen original para visualizar\n",
        "        image_path = dataset.ids[idx]\n",
        "        image_file = dataset.coco.loadImgs(image_path)[0]['file_name']\n",
        "        image = Image.open(os.path.join(img_dir, image_file)).convert(\"RGB\")\n",
        "\n",
        "        # Dibujar predicciones\n",
        "        result_img = draw_boxes(image.copy(), output)\n",
        "\n",
        "        # Guardar\n",
        "        result_img.save(os.path.join(out_dir, f\"prediction_{idx+1}.jpg\"))"
      ],
      "metadata": {
        "id": "M4FSX1fJ9m5j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}